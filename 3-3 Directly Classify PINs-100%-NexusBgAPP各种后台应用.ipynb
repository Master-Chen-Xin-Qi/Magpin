{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# 由于文件夹处理流程复杂且不一致；\n",
    "# 所以单独将PIN分类列出\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读所有mag数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All folders: \n",
      " ['collectCalls_1', 'collectCalls_2', 'collectDownload_1', 'collectDownload_2', 'collectLight_1', 'collectLight_2', 'collectMusic_1', 'collectMusic_2', 'collectStatic_1', 'collectStatic_2', 'collectVideoRecord_1', 'collectVideoRecord_2'] \n",
      "Name folders:\n",
      " ['collectCalls', 'collectDownload', 'collectLight', 'collectMusic', 'collectStatic', 'collectVideoRecord']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, pandas as pd\n",
    "\n",
    "def get_Hyperparas():\n",
    "    \n",
    "    ''' 在此处最终定义几个超参数  '''\n",
    "    # 定义备选的超参数\n",
    "    mag_file = {'Huawei1':[\"Time\", \"3\", \"AcceX\", \"AcceY\", \"AcceZ\", \"5\", \"MagX\", \"MagY\", \"MagZ\"],\n",
    "               'Huawei2':[\"Time\",\"MagX\", \"MagY\", \"MagZ\" ],\n",
    "               'OPPO':[ \"Time\", \"3\", \"AcceX\", \"AcceY\", \"AcceZ\", \"4\", \"GyroX\", \"GyroY\", \"GyroZ\", \"5\", \"MagX\", \"MagY\", \"MagZ\"],\n",
    "               'Nexus':[ \"Time\", \"3\", \"AcceX\", \"AcceY\", \"AcceZ\", \"4\", \"GyroX\", \"GyroY\", \"GyroZ\", \"5\", \"MagX\", \"MagY\", \"MagZ\"]}\n",
    "    base_folder_pin = 'D:/eclipse/workspace/TheGreatTrials/read/Huawei-1103-PIN/'\n",
    "    base_folder_color = 'D:/eclipse/workspace/TheGreatTrials/read/Huawei-1103-Color/'\n",
    "    base_folder_color_OPPO = 'D:/eclipse/workspace/TheGreatTrials/read/OPPO-1102-color/'\n",
    "    base_folder_color_Nexus = 'D:/eclipse/workspace/TheGreatTrials/read/Nexus-1101-Color/'\n",
    "    base_folder_Bg_NexusBgAPP_Double = 'D:/eclipse/workspace/TheGreatTrials/read/Nexus-1031-BgAPP-Double/'\n",
    "    # 最终选择参数\n",
    "    mag_columns = mag_file['Nexus']\n",
    "    base_folder = base_folder_Bg_NexusBgAPP_Double # base_folder_color\n",
    "    data_folders,examples = get_keywords(base_folder) # Get folders\n",
    "    return mag_columns,base_folder,data_folders,examples\n",
    "\n",
    "def get_keywords(base_folder):\n",
    "    all_folders = []\n",
    "    names_folder = []\n",
    "    for folder in os.listdir(base_folder):\n",
    "        if '.' not in folder and 'plot' not in folder: # 剔除文件夹下的readMe.txt和其他plot文件夹\n",
    "            all_folders.append(folder) # 拿到所有文件夹\n",
    "        if '_' in folder and folder[:-2] not in names_folder: #只以数字分隔\n",
    "            names_folder.append(folder[:-2])\n",
    "    print('All folders: \\n',all_folders,'\\nName folders:\\n',names_folder)\n",
    "    return all_folders, names_folder\n",
    "\n",
    "# 读一个文件夹，并取出所有文件，并命名所有列\n",
    "def read_a_folder(data_folder):\n",
    "    # Read,Rename\n",
    "    all_data = {}\n",
    "    data_info = pd.read_csv(data_folder + 'magSensors.txt')\n",
    "    #print( len(mag_columns) , len(data_info.iloc[0]) )\n",
    "    assert len(mag_columns) == len(data_info.iloc[0]) # 文件读取列数和命名所需列数相等\n",
    "    data_info.columns = mag_columns\n",
    "    import copy\n",
    "    ttt = copy.copy(mag_columns)\n",
    "    ttt.remove('Time')\n",
    "    ttt.remove('3')\n",
    "    if '4' in mag_columns:\n",
    "        ttt.remove('4')\n",
    "        ttt.remove('5')\n",
    "        data_info = data_info[ttt]\n",
    "    return data_info\n",
    "\n",
    "# 取得超参数\n",
    "mag_columns,base_folder,data_folders,examples = get_Hyperparas()\n",
    "\n",
    "# 合并每个文件夹里的文件\n",
    "final_info = {}\n",
    "for folder in data_folders:\n",
    "    #print('%s' % folder)\n",
    "    all_data = read_a_folder(base_folder + folder + '/')\n",
    "    final_info[folder] = all_data\n",
    "    \n",
    "#如某类别含多个文件夹，就合并文件\n",
    "use_mergeFiles = True\n",
    "if use_mergeFiles:\n",
    "    finally_you_win = {}\n",
    "    for example in examples:\n",
    "        if example + '_1' in final_info.keys():\n",
    "            tttt1 = final_info[example + '_1']\n",
    "            tttt2 = final_info[example + '_2']\n",
    "            tttt3 = pd.concat([tttt1, tttt2], axis=0)\n",
    "        else:\n",
    "            tttt1 = final_info[example + '_1']\n",
    "            tttt3 = tttt1\n",
    "        finally_you_win[example] = tttt3\n",
    "final_info = finally_you_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 合并完毕，开始分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conccat with labels, the file has shape: (183, 1000)  After: (183, 1001)\n",
      "Processing file collectLight It has shape (183, 1001) labeled with [ 0.]\n",
      "Before conccat with labels, the file has shape: (180, 1000)  After: (180, 1001)\n",
      "Processing file collectMusic It has shape (180, 1001) labeled with [ 1.]\n",
      "Before conccat with labels, the file has shape: (163, 1000)  After: (163, 1001)\n",
      "Processing file collectCalls It has shape (163, 1001) labeled with [ 2.]\n",
      "Before conccat with labels, the file has shape: (42, 1000)  After: (42, 1001)\n",
      "Processing file collectVideoRecord It has shape (42, 1001) labeled with [ 3.]\n",
      "Before conccat with labels, the file has shape: (179, 1000)  After: (179, 1001)\n",
      "Processing file collectStatic It has shape (179, 1001) labeled with [ 4.]\n",
      "Before conccat with labels, the file has shape: (156, 1000)  After: (156, 1001)\n",
      "Processing file collectDownload It has shape (156, 1001) labeled with [ 5.]\n",
      "Before conccat with labels, the file has shape: (183, 1000)  After: (183, 1001)\n",
      "Processing file collectLight It has shape (183, 1001) labeled with [ 0.]\n",
      "Before conccat with labels, the file has shape: (180, 1000)  After: (180, 1001)\n",
      "Processing file collectMusic It has shape (180, 1001) labeled with [ 1.]\n",
      "Before conccat with labels, the file has shape: (163, 1000)  After: (163, 1001)\n",
      "Processing file collectCalls It has shape (163, 1001) labeled with [ 2.]\n",
      "Before conccat with labels, the file has shape: (42, 1000)  After: (42, 1001)\n",
      "Processing file collectVideoRecord It has shape (42, 1001) labeled with [ 3.]\n",
      "Before conccat with labels, the file has shape: (179, 1000)  After: (179, 1001)\n",
      "Processing file collectStatic It has shape (179, 1001) labeled with [ 4.]\n",
      "Before conccat with labels, the file has shape: (156, 1000)  After: (156, 1001)\n",
      "Processing file collectDownload It has shape (156, 1001) labeled with [ 5.]\n",
      "Before conccat with labels, the file has shape: (183, 1000)  After: (183, 1001)\n",
      "Processing file collectLight It has shape (183, 1001) labeled with [ 0.]\n",
      "Before conccat with labels, the file has shape: (180, 1000)  After: (180, 1001)\n",
      "Processing file collectMusic It has shape (180, 1001) labeled with [ 1.]\n",
      "Before conccat with labels, the file has shape: (163, 1000)  After: (163, 1001)\n",
      "Processing file collectCalls It has shape (163, 1001) labeled with [ 2.]\n",
      "Before conccat with labels, the file has shape: (42, 1000)  After: (42, 1001)\n",
      "Processing file collectVideoRecord It has shape (42, 1001) labeled with [ 3.]\n",
      "Before conccat with labels, the file has shape: (179, 1000)  After: (179, 1001)\n",
      "Processing file collectStatic It has shape (179, 1001) labeled with [ 4.]\n",
      "Before conccat with labels, the file has shape: (156, 1000)  After: (156, 1001)\n",
      "Processing file collectDownload It has shape (156, 1001) labeled with [ 5.]\n",
      "Before conccat with labels, the file has shape: (183, 1000)  After: (183, 1001)\n",
      "Processing file collectLight It has shape (183, 1001) labeled with [ 0.]\n",
      "Before conccat with labels, the file has shape: (180, 1000)  After: (180, 1001)\n",
      "Processing file collectMusic It has shape (180, 1001) labeled with [ 1.]\n",
      "Before conccat with labels, the file has shape: (163, 1000)  After: (163, 1001)\n",
      "Processing file collectCalls It has shape (163, 1001) labeled with [ 2.]\n",
      "Before conccat with labels, the file has shape: (42, 1000)  After: (42, 1001)\n",
      "Processing file collectVideoRecord It has shape (42, 1001) labeled with [ 3.]\n",
      "Before conccat with labels, the file has shape: (179, 1000)  After: (179, 1001)\n",
      "Processing file collectStatic It has shape (179, 1001) labeled with [ 4.]\n",
      "Before conccat with labels, the file has shape: (156, 1000)  After: (156, 1001)\n",
      "Processing file collectDownload It has shape (156, 1001) labeled with [ 5.]\n",
      "Finally we combine data and got shape: (3612, 1001) Labels: Counter({0.0: 732, 1.0: 720, 4.0: 716, 2.0: 652, 5.0: 624, 3.0: 168})\n"
     ]
    }
   ],
   "source": [
    "# 本部分三个超参数： 1. 怎样切行   2. 怎样切列  3.怎样进行交叉验证\n",
    "DevideParts = 4\n",
    "sampele_rate = 2\n",
    "rows_windowSize, rows_overlapSize = 200, 180\n",
    "cols_usage = ['MagX', 'MagY','MagZ','MagX', 'MagZ']\n",
    "\n",
    "all_division_files = []\n",
    "# select divide\n",
    "for i in range(DevideParts):\n",
    "    label_number = 0\n",
    "    # select file\n",
    "    for keyword in final_info.keys():\n",
    "        if 'recordAudio' not in keyword:\n",
    "            # select rows\n",
    "            one_category_data = final_info[keyword]\n",
    "            # select cols\n",
    "            #one_category_data = one_category_data['MagX']+one_category_data['MagY']+one_category_data['MagZ']\n",
    "            one_category_data = one_category_data[cols_usage]\n",
    "            # Resample\n",
    "            locs = np.arange(0,len(one_category_data),sampele_rate)\n",
    "            one_category_data = one_category_data.iloc[locs]\n",
    "            cols = 1 if type(\n",
    "                one_category_data) == pd.Series else one_category_data.shape[1]\n",
    "            # select divide\n",
    "            this_division_use_rows = len(one_category_data) // DevideParts\n",
    "            one_category_part_data = one_category_data.iloc[\n",
    "                i * this_division_use_rows:(i + 1) * this_division_use_rows]\n",
    "            # select rows\n",
    "            all_window_list_this_division = []\n",
    "            window_no_overlap = rows_windowSize - rows_overlapSize\n",
    "            count_windows = 0\n",
    "            while window_no_overlap * count_windows + rows_windowSize < this_division_use_rows:\n",
    "                window_data = one_category_part_data.iloc[window_no_overlap *\n",
    "                                                          count_windows:\n",
    "                                                          (window_no_overlap *\n",
    "                                                           count_windows +\n",
    "                                                           rows_windowSize)]\n",
    "                #print(window_data.shape)\n",
    "                all_window_list_this_division.extend(window_data.values)\n",
    "                count_windows += 1\n",
    "            # Generate one file all data\n",
    "            all_window_array = np.array(all_window_list_this_division).\\\n",
    "                        reshape([-1,rows_windowSize*cols])\n",
    "            labels = np.ones([count_windows, 1]) * label_number\n",
    "            one_file_all = np.concatenate([all_window_array, labels], axis=1)\n",
    "            print('Before conccat with labels, the file has shape:',\n",
    "                  all_window_array.shape, ' After:', one_file_all.shape)\n",
    "            label_number += 1\n",
    "            # iter over files\n",
    "            print('Processing file', keyword, 'It has shape',\n",
    "                  one_file_all.shape, 'labeled with', labels[-1])\n",
    "            all_division_files.extend(one_file_all)\n",
    "    i += 1\n",
    "\n",
    "# Finally concat data\n",
    "from collections import Counter\n",
    "all_division_files = np.array(all_division_files).reshape(\n",
    "    [-1, 1 + rows_windowSize * cols])\n",
    "print('Finally we combine data and got shape:', all_division_files.shape,\n",
    "      'Labels:', Counter(all_division_files[:, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after PCA: (3612, 1000)\n",
      "Counter({0.0: 183, 1.0: 180, 4.0: 179, 2.0: 163, 5.0: 156, 3.0: 42}) \n",
      " Counter({0.0: 183, 1.0: 180, 4.0: 179, 2.0: 163, 5.0: 156, 3.0: 42})\n",
      "0.919158361019\n",
      "\n",
      " [[183   0   0   0   0   0]\n",
      " [  0 167   0  12   1   0]\n",
      " [  0   0 163   0   0   0]\n",
      " [  0   0   0  24   1  17]\n",
      " [  0   0   0   0 179   0]\n",
      " [  0   0   0   0  42 114]]\n",
      "Counter({0.0: 366, 1.0: 360, 4.0: 358, 2.0: 326, 5.0: 312, 3.0: 84}) \n",
      " Counter({0.0: 183, 1.0: 180, 4.0: 179, 2.0: 163, 5.0: 156, 3.0: 42})\n"
     ]
    }
   ],
   "source": [
    "#all_division_files = all_division_files[np.where(all_division_files[:,-1] != 1)[0]]\n",
    "#print(len(all_division_files))\n",
    "\n",
    "feature = all_division_files[:, :-1]\n",
    "label = all_division_files[:, -1]\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.995)\n",
    "pca.fit(feature)\n",
    "#feature = pca.transform(feature)\n",
    "print('Data shape after PCA:', feature.shape)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=DevideParts - 1)\n",
    "\n",
    "cf1 = []\n",
    "for train_index, test_index in tscv.split(feature):\n",
    "#for tmd in range(1):\n",
    "\n",
    "    # 划分\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #train_X,test_X, train_y, test_y = train_test_split(feature,label,test_size = 0.5,random_state = 0, shuffle=False)\n",
    "    from collections import Counter\n",
    "    train_X, test_X = feature[train_index], feature[test_index]\n",
    "    train_y, test_y = label[train_index], label[test_index]\n",
    "\n",
    "    #print('Train:', np.min(train_index), np.max(train_index), 'Test:',\n",
    "    #      np.min(test_index), np.max(test_index))\n",
    "    print(Counter(train_y), '\\n', Counter(test_y))\n",
    "    #print(train_index,test_index)\n",
    "\n",
    "    # 分类\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=500,\n",
    "                                 max_depth=50,\n",
    "                                 random_state=0)\n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    # 分类\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "    neigh.fit(train_X, train_y)\n",
    "\n",
    "    model = clf\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(clf, \"RF_realTime.m\")\n",
    "\n",
    "    # 评价\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    predict_y = model.predict(test_X)\n",
    "    confusion = confusion_matrix(test_y, predict_y)\n",
    "    cf1.append(confusion)\n",
    "    print(accuracy_score(test_y, predict_y))\n",
    "    print('\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cf1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16136/3524434914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mcfion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcfion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cf1' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "use_average_rows = 3\n",
    "# Plot\n",
    "normalize = True\n",
    "cmap = plt.cm.Reds \n",
    "font_size = 22  # 15\n",
    "algo_fontsize = 25  # 20\n",
    "un_fig_size = (20, 8)\n",
    "un_app_fig_size = (12, 7)\n",
    "fig_size = (10,8)\n",
    "confusion_fig_size = (10, 6)\n",
    "window_fig_size = (15, 8)\n",
    "precise_rate = \"{:0.2f}\"\n",
    "def plot_action_cf(df_cm, title_, figure_saved, target_names, normalize, rotation,use_notation,font_size):\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    fig.set_tight_layout({'pad': 1})\n",
    "    ax = sn.heatmap(ax=ax, data=df_cm, annot=use_notation, cmap=cmap, vmin=0, vmax=1,\n",
    "                    fmt=\".2f\", annot_kws={\"size\": font_size - 15})\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    # here set the labelsize by 20\n",
    "    cbar.ax.tick_params(labelsize=font_size - 15)  # colorbar\n",
    "    ax.tick_params(labelsize=font_size - 15, width=1)\n",
    "    ax.set_aspect(1)\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(0, len(target_names)) + 0.5 \n",
    "        plt.xticks(tick_marks, target_names,fontsize=font_size-10, rotation=rotation) # [1,2,3,4,5,6,7,8,9,10,11]\n",
    "        plt.yticks(tick_marks, target_names, rotation=0, fontsize=font_size-10)\n",
    "    plt.savefig(base_folder + figure_saved + '.eps', format='eps')    \n",
    "    plt.savefig(base_folder + figure_saved + '.pdf', format='pdf')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "print(cf1[2])\n",
    "cfion = np.array(cf1[1])\n",
    "print('\\n',cfion)\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.reset() #系统默认选项\n",
    "# 绘制\n",
    "title_ = 'Classification of different Apps'\n",
    "figure_saved = 'PIN分类结果'\n",
    "\n",
    "# 转换混淆矩阵\n",
    "final_confusion = []\n",
    "arra1 = cf1[1]\n",
    "sum_rows = np.sum(arra1,axis=1)\n",
    "for i in range(len(arra1)):\n",
    "    print(i,sum_rows[i])\n",
    "    tmp = arra1[i] / sum_rows[i]\n",
    "    final_confusion.append(tmp)\n",
    "final_confusion = np.array(final_confusion)\n",
    "cfion = np.round(final_confusion,3)\n",
    "\n",
    "labels = list(final_info.keys())\n",
    "labels = examples\n",
    "plot_action_cf(cfion, title_, figure_saved, labels, normalize=False, rotation=45,use_notation=True,font_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
